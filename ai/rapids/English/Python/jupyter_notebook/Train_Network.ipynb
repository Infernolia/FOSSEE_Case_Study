{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../Start_Here.ipynb)\n",
    "\n",
    "[Previous Notebook](Approach_to_the_Problem.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[1](Problem_statement.ipynb)\n",
    "[2](Approach_to_the_Problem.ipynb)\n",
    "[3]\n",
    "[4](Visualising.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](Visualising.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost - Part 3 \n",
    "\n",
    "**Contents of the This Notebook:**\n",
    "\n",
    "- [Getting Started](#Getting-Started)\n",
    "- [AEV Computer and Loading data](#AEV-Computer-and-Loading-data)\n",
    "- [Defining Networks for Atoms and Loss Function](#Defining-Networks-for-Atoms-and-Loss-Function)\n",
    "- [Setting Hyper Parameters](#Setting-Hyper-Parameters)\n",
    "- [Training](#Training)\n",
    "- [Visualisations]()\n",
    "\n",
    "\n",
    "**By the End of this Notebook you will:**\n",
    "\n",
    "- Define and Train the Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Let us start this off by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Import Required Modules\n",
    "import torch\n",
    "import torchani\n",
    "import os\n",
    "import math\n",
    "import torch.utils.tensorboard\n",
    "import tqdm\n",
    "\n",
    "# device to run the training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEV Computer and Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to construct the AEV Computer using certain values calculated from the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Parameters Computer from the ANI1 Dataset\n",
    "Rcr = 5.2000e+00\n",
    "Rca = 3.5000e+00\n",
    "EtaR = torch.tensor([1.6000000e+01], device=device)\n",
    "ShfR = torch.tensor([9.0000000e-01, 1.1687500e+00, 1.4375000e+00, 1.7062500e+00, 1.9750000e+00, 2.2437500e+00, 2.5125000e+00, 2.7812500e+00, 3.0500000e+00, 3.3187500e+00, 3.5875000e+00, 3.8562500e+00, 4.1250000e+00, 4.3937500e+00, 4.6625000e+00, 4.9312500e+00], device=device)\n",
    "Zeta = torch.tensor([3.2000000e+01], device=device)\n",
    "ShfZ = torch.tensor([1.9634954e-01, 5.8904862e-01, 9.8174770e-01, 1.3744468e+00, 1.7671459e+00, 2.1598449e+00, 2.5525440e+00, 2.9452431e+00], device=device)\n",
    "EtaA = torch.tensor([8.0000000e+00], device=device)\n",
    "ShfA = torch.tensor([9.0000000e-01, 1.5500000e+00, 2.2000000e+00, 2.8500000e+00], device=device)\n",
    "num_species = 4\n",
    "\n",
    "# Setup AEV Computer\n",
    "aev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\n",
    "\n",
    "# Setting Parameter to `None` will allow EnergyShifter to Calculate Self Energy of the Atoms\n",
    "energy_shifter = torchani.utils.EnergyShifter(None)\n",
    "\n",
    "#Convert Chemical Symbols to Integers\n",
    "species_to_tensor = torchani.utils.ChemicalSymbolsToInts('HCNO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that we need to subtracting energies by the self energies of all\n",
    "atoms for each molecule. This makes the range of energies in a reasonable\n",
    "range. The second argument defines how to convert species as a list of string\n",
    "to tensor, that is, for all supported chemical symbols, which is correspond to\n",
    "``0``, which correspond to ``1``, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    path = os.path.dirname(os.path.realpath(__file__))\n",
    "except NameError:\n",
    "    path = os.getcwd()\n",
    "dspath = os.path.join(path, 'ANI1_release/ani_gdb_s04.h5')\n",
    "\n",
    "batch_size = 2560\n",
    "\n",
    "training, validation = torchani.data.load_ani_dataset(\n",
    "    dspath, species_to_tensor, batch_size, rm_outlier=True, device=device,\n",
    "    transform=[energy_shifter.subtract_from_dataset], split=[0.8, None])\n",
    "\n",
    "print('Self atomic energies: ', energy_shifter.self_energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When iterating the dataset, we will get pairs of input and output\n",
    " ``(species_coordinates, properties)``, where ``species_coordinates`` is the\n",
    " input and ``properties`` is the output.\n",
    "\n",
    " ``species_coordinates`` is a list of species-coordinate pairs, with shape\n",
    " ``(N, Na)`` and ``(N, Na, 3)``. The reason for getting this type is, when\n",
    " loading the dataset and generating minibatches, the whole dataset are\n",
    " shuffled and each minibatch contains structures of molecules with a wide\n",
    " range of number of atoms. Molecules of different number of atoms are batched\n",
    " into single by padding. The way padding works is: adding ghost atoms, with\n",
    " species 'X', and do computations as if they were normal atoms. But when\n",
    " computing AEVs, atoms with species `X` would be ignored. To avoid computation\n",
    " wasting on padding atoms, minibatches are further splitted into chunks. Each\n",
    " chunk contains structures of molecules of similar size, which minimize the\n",
    " total number of padding atoms required to add. The input list\n",
    " ``species_coordinates`` contains chunks of that minibatch we are getting. The\n",
    " batching and chunking happens automatically, so the user does not need to\n",
    " worry how to construct chunks, but the user need to compute the energies for\n",
    " each chunk and concat them into single tensor.\n",
    "\n",
    " The output, i.e. ``properties`` is a dictionary holding each property. This\n",
    " allows us to extend TorchANI in the future to training forces and properties.\n",
    "\n",
    "\n",
    "Now let's define atomic neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Networks for Atoms and Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the Mean Squared Error Loss from the PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is Simple Feed-forward Network with Inputs as 384 and outputs the Energy of the Atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(384, 160),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(160, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "C_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(384, 144),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(144, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "N_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(384, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "O_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(384, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "nn = torchani.ANIModel([H_network, C_network, N_network, O_network])\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiliase Weights and biases :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us Now Initialise the Weights and biases\n",
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "nn.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a pipeline of AEV Computer --> Neural Networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchani.nn.Sequential(aev_computer, nn).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup the optimizers. NeuroChem uses Adam with decoupled weight decay\n",
    "to updates the weights and Stochastic Gradient Descent (SGD) to update the biases.\n",
    "Moreover, we need to specify different weight decay rate for different layes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW = torchani.optim.AdamW([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].weight]},\n",
    "    {'params': [H_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [H_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [H_network[6].weight]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].weight]},\n",
    "    {'params': [C_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [C_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [C_network[6].weight]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].weight]},\n",
    "    {'params': [N_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [N_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [N_network[6].weight]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].weight]},\n",
    "    {'params': [O_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [O_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [O_network[6].weight]},\n",
    "])\n",
    "\n",
    "SGD = torch.optim.SGD([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].bias]},\n",
    "    {'params': [H_network[2].bias]},\n",
    "    {'params': [H_network[4].bias]},\n",
    "    {'params': [H_network[6].bias]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].bias]},\n",
    "    {'params': [C_network[2].bias]},\n",
    "    {'params': [C_network[4].bias]},\n",
    "    {'params': [C_network[6].bias]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].bias]},\n",
    "    {'params': [N_network[2].bias]},\n",
    "    {'params': [N_network[4].bias]},\n",
    "    {'params': [N_network[6].bias]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].bias]},\n",
    "    {'params': [O_network[2].bias]},\n",
    "    {'params': [O_network[4].bias]},\n",
    "    {'params': [O_network[6].bias]},\n",
    "], lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a learning rate scheduler to do learning rate decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamW, factor=0.5, patience=100, threshold=0)\n",
    "SGD_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD, factor=0.5, patience=100, threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by minimizing the MSE loss, until validation RMSE no longer\n",
    "improves during a certain number of steps, decay the learning rate and repeat\n",
    "the same process, stop until the learning rate is smaller than a threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need to validate on validation set and if validation error\n",
    "is better than the best, then save the new best model to a checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert energy unit from Hartree to kcal/mol\n",
    "def hartree2kcal(x):\n",
    "    return 627.509 * x\n",
    "\n",
    "\n",
    "def validate():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    total_mse = 0.0\n",
    "    count = 0\n",
    "    for batch_x, batch_y in validation:\n",
    "        true_energies = batch_y['energies']\n",
    "        predicted_energies = []\n",
    "        for chunk_species, chunk_coordinates in batch_x:\n",
    "            chunk_energies = model((chunk_species, chunk_coordinates)).energies\n",
    "            predicted_energies.append(chunk_energies)\n",
    "        predicted_energies = torch.cat(predicted_energies)\n",
    "        total_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "        count += predicted_energies.shape[0]\n",
    "    return hartree2kcal(math.sqrt(total_mse / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use TensorBoard to visualize our training process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = torch.utils.tensorboard.SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we come to the training loop.\n",
    "\n",
    "In this tutorial, we are setting the maximum epoch to a small number,\n",
    "only to make this demo terminate fast. For proper training, this should be\n",
    "set to a much larger value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training starts here\n",
    "print(\"training starting from epoch\", 0 + 1)\n",
    "# Set Parameters - Max_Epochs & Early Stopping \n",
    "max_epochs = 10\n",
    "early_stopping_learning_rate = 1.0E-5\n",
    "# Store the Best Model as a Checkpoint\n",
    "best_model_checkpoint = 'best.pt'\n",
    "\n",
    "# Loop through the Training Epoch\n",
    "for _ in range(AdamW_scheduler.last_epoch + 1, max_epochs):\n",
    "    rmse = validate()\n",
    "    print('RMSE:', rmse, 'at epoch', AdamW_scheduler.last_epoch + 1)\n",
    "\n",
    "    learning_rate = AdamW.param_groups[0]['lr']\n",
    "    \n",
    "    # Stop training if Learning Rate is Less than Early Stopping Rate \n",
    "    if learning_rate < early_stopping_learning_rate:\n",
    "        break\n",
    "\n",
    "    # Store the Best Model\n",
    "    if AdamW_scheduler.is_better(rmse, AdamW_scheduler.best):\n",
    "        torch.save(nn.state_dict(), best_model_checkpoint)\n",
    "    \n",
    "    # Learning Rate Scheduler \n",
    "    AdamW_scheduler.step(rmse)\n",
    "    SGD_scheduler.step(rmse)\n",
    "    \n",
    "    #Add Parameters to Tensorbaord \n",
    "    tensorboard.add_scalar('validation_rmse', rmse, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('best_validation_rmse', AdamW_scheduler.best, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('learning_rate', learning_rate, AdamW_scheduler.last_epoch)\n",
    "    \n",
    "    # Loop through the batch of Data\n",
    "    for i, (batch_x, batch_y) in tqdm.tqdm(\n",
    "        enumerate(training),\n",
    "        total=len(training),\n",
    "        desc=\"epoch {}\".format(AdamW_scheduler.last_epoch)\n",
    "    ):\n",
    "        # Store Predicted Values\n",
    "        true_energies = batch_y['energies']\n",
    "        predicted_energies = []\n",
    "        num_atoms = []\n",
    "\n",
    "        for chunk_species, chunk_coordinates in batch_x:\n",
    "            num_atoms.append((chunk_species >= 0).to(true_energies.dtype).sum(dim=1))\n",
    "            chunk_energies = model((chunk_species, chunk_coordinates)).energies\n",
    "            predicted_energies.append(chunk_energies)\n",
    "\n",
    "        num_atoms = torch.cat(num_atoms)\n",
    "        predicted_energies = torch.cat(predicted_energies)\n",
    "        loss = (mse(predicted_energies, true_energies) / num_atoms.sqrt()).mean()\n",
    "        \n",
    "        # Zero out the Gradients and Run Backpropgation. \n",
    "        AdamW.zero_grad()\n",
    "        SGD.zero_grad()\n",
    "        loss.backward()\n",
    "        AdamW.step()\n",
    "        SGD.step()\n",
    "\n",
    "        # write current batch loss to TensorBoard\n",
    "        tensorboard.add_scalar('batch_loss', loss, AdamW_scheduler.last_epoch * len(training) + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Visulisations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `Tensorboard` from Jupyter Home to analyse training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the Next Notebook , Let us see how this model inference is used in Molecular Dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Notebook](Approach_to_the_Problem.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[1](Problem_statement.ipynb)\n",
    "[2](Approach_to_the_Problem.ipynb)\n",
    "[3]\n",
    "[4](Visualising.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](Visualising.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../Start_Here.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
